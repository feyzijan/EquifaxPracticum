{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This code uses selenium and requests to scrape the contents of the top 5 websites found in the bing web search results for each Firm_Name and Field. It reads in data from \"firms_web_search_results.db\" and stores website scraping results in \"firms_web_search_website_scrapings.db\".\n",
    "- It will skip over fields that are already populated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import sqlite3\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.exceptions import Timeout, RequestException\n",
    "\n",
    "from gemini_prompts import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Define Class for webscraping\n",
    "- Uses requests and selenium to scrape a website, and returns the text with the longest length\n",
    "- *IMPORTANT*: Update CHROMEDRIVER_PATH with the path to your chrome driver installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_web_result = \"No website scrapings found\"\n",
    "class SeleniumExtractionError(Exception):\n",
    "    \"\"\"Custom exception for Selenium extraction errors.\"\"\"\n",
    "    pass\n",
    "\n",
    "CHROMEDRIVER_PATH = '/opt/homebrew/bin/chromedriver'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebScraper():\n",
    "    def __init__(self) -> None:\n",
    "        # Set up Chrome driver with webdriver manager\n",
    "        self.options = webdriver.ChromeOptions()\n",
    "        self.options.add_argument('--headless')  # Run headless for no browser window\n",
    "        self.options.add_argument('--disable-gpu')  # Disable GPU acceleration\n",
    "        self.options.add_argument('--no-sandbox')  # Required for some Linux environments\n",
    "        self.options.add_argument('--disable-extensions')\n",
    "        self.options.add_argument('--disable-plugins')\n",
    "        self.options.add_argument('--disable-images')  # Prevent loading images to save bandwidth\n",
    "        self.options.add_argument('--disable-browser-side-navigation')\n",
    "        self.options.add_argument('--mute-audio') \n",
    "        self.options.page_load_strategy = 'eager'  \n",
    "\n",
    "        # Paremeters for requests\n",
    "        self.requests_headers =  {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "        }\n",
    "        self.requests_timeout = 4\n",
    "\n",
    "        self.n_webpages_to_scrape = 5\n",
    "        self.webpages = {}\n",
    "        self.service = Service(executable_path=CHROMEDRIVER_PATH)\n",
    "\n",
    "\n",
    "    def get_top_webpages(self,web_search_results: dict) -> dict:\n",
    "        '''\n",
    "        Get the top n webpages' names and urls from a given Bing search result\n",
    "        Returns dict with 'site_name': 'url' pairs\n",
    "        '''\n",
    "        self.webpages = {}\n",
    "        print(\"Getting urls of top webpages\")\n",
    "    \n",
    "        # get dict of site names and urls\n",
    "        if 'webPages' not in web_search_results.keys():\n",
    "            print(\"No webpages found\")\n",
    "            return self.webpages\n",
    "        else:\n",
    "            for result in web_search_results['webPages']['value']:\n",
    "                # print(\"Result is \", result)\n",
    "                # print(result[\"siteName\"])\n",
    "                if \"siteName\" in result.keys():\n",
    "                    self.webpages[result[\"siteName\"]] = result[\"url\"]\n",
    "                elif \"name\" in result.keys():\n",
    "                    self.webpages[result[\"name\"]] = result[\"url\"]\n",
    "                else:\n",
    "                    pass\n",
    "                if len(self.webpages) >= self.n_webpages_to_scrape:\n",
    "                    break\n",
    "                \n",
    "            # print(\"debug: length of webpages is \", len(self.webpages))\n",
    "            return self.webpages\n",
    "        \n",
    "    def extract_text_with_selenium(self,url):\n",
    "        '''\n",
    "        Extract contents of given url with Selenium with max 5s timeout\n",
    "        Returns text if successful, None otherwise\n",
    "        '''\n",
    "        self.driver = webdriver.Chrome(service=self.service, options=self.options)\n",
    "        self.driver.set_page_load_timeout(4)\n",
    "        try:\n",
    "            # Open the URL in the browser\n",
    "            # print(\"Selenium DEBUG: getting url\")\n",
    "            self.driver.get(url) # KEEPS GETTING STUCK HERE!!!!\n",
    "            time.sleep(0.1) # this counteracts some automatic blocking\n",
    "\n",
    "            # Wait for the page body to be present (max 5 seconds)\n",
    "            WebDriverWait(self.driver, 5).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
    "            # print(\"Selenium DEBUG: found body\")\n",
    "            # Get page source and parse it after it has fully loaded\n",
    "            page_source = self.driver.page_source\n",
    "            # print(\"Selenium DEBUG: found page source\")\n",
    "            soup = BeautifulSoup(page_source, 'html.parser')\n",
    "            text = soup.get_text(separator='\\n')\n",
    "\n",
    "            self.safe_quit_selenium()\n",
    "            # self.safe_quit_selenium()\n",
    "            return text\n",
    "        \n",
    "        except TimeoutException:\n",
    "            error_message = \"Page load exceeded time limit of 5 seconds\"\n",
    "            print(f\"An error occurred with Selenium:: {error_message}\")\n",
    "            # self.safe_quit_selenium()\n",
    "            return \"Page contents not loaded in time\"\n",
    "\n",
    "        except Exception as e:\n",
    "            error_message = str(e)\n",
    "            print(f\"An error occurred with Selenium: {error_message}\")\n",
    "            self.safe_quit_selenium()\n",
    "            if \"disconnected: not connected to DevTools\" in error_message:\n",
    "                print(f\"Error occurred: {error_message}\")\n",
    "                raise SeleniumExtractionError(f\"DevTools disconnection error for URL: {url}\")\n",
    "            else:\n",
    "                raise SeleniumExtractionError(f\"An unexpected error occurred while extracting: {error_message}\")\n",
    "    \n",
    "    def safe_quit_selenium(self):\n",
    "        try:\n",
    "            self.driver.quit()\n",
    "        except Exception as e:\n",
    "            print(f\"Error during driver quit: {e}\")\n",
    "            \n",
    "        \n",
    "    def extract_text_with_requests(self,url):\n",
    "        ''' Extract the url with requests and return the text if successfull, None if not'''\n",
    "        try:\n",
    "            # Fetch the content from the URL with headers\n",
    "            response = requests.get(url, headers=self.requests_headers, timeout=self.requests_timeout)\n",
    "            response.raise_for_status()  # Check if the request was successful\n",
    "            # Parse text\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            text = soup.get_text(separator='\\n')  # Using '\\n' to preserve some structure\n",
    "            return text\n",
    "\n",
    "        except Timeout:\n",
    "            print(f\"Request timed out after 5 seconds for URL: {url}\")\n",
    "            return None\n",
    "        except RequestException as e:\n",
    "            print(f\"Requests Error fetching the URL {url}: {e}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"Requests Unexpected error occurred while processing {url}: {e}\")\n",
    "            return None\n",
    "        \n",
    "    def scrape_webpage(self, url: str) -> str:\n",
    "        ''' Scrape the given url using selenium and requests, return the longer text'''\n",
    "\n",
    "        # print(\"Extracting webpage with selenium\")\n",
    "        # print(\"---Debug: extracting webpage with url\", url)\n",
    "        text_selenium = self.extract_text_with_selenium(url)\n",
    "        text_requests = self.extract_text_with_requests(url)\n",
    "\n",
    "        texts = [text for text in [text_selenium, text_requests] if text]\n",
    "        if not texts:\n",
    "            return no_web_result\n",
    "\n",
    "        # Return the text with more content\n",
    "        longest_text = max(texts, key=len)\n",
    "        return self.clean_text(longest_text)\n",
    "            \n",
    "    def clean_text(self,text):\n",
    "        ''' Cleans result text '''\n",
    "        if text is None:\n",
    "            return None\n",
    "        # replace multiple newlines with a single one\n",
    "        text = re.sub(r'\\n+', '\\n', text)\n",
    "        # remove leading/trailing whitespaces\n",
    "        text = '\\n'.join(line.strip() for line in text.splitlines())\n",
    "        # Remove extra spaces between words\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text\n",
    "    \n",
    "# Use this to return results for parallel processing\n",
    "def scrape_website(scraper_instance, website_name, website_url):\n",
    "    print(f\"Getting Contents of the website of {website_name} with url {website_url}\")\n",
    "    return website_name, scraper_instance.scrape_webpage(website_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Connect to the source sqlite table that stores web search results, and the target table to store website scrapings\n",
    "- Note: Download the latest firms_web_search_results.db from Google drive (In the datasets folder), and update there when done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_websearch = sqlite3.connect(\"firms_web_search_results.db\")\n",
    "conn_websites = sqlite3.connect(\"firms_web_search_website_scrapings.db\")\n",
    "cursor_websearch = conn_websearch.cursor()\n",
    "cursor_websites = conn_websites.cursor()\n",
    "\n",
    "cursor_websites.execute('''\n",
    "CREATE TABLE IF NOT EXISTS firms_web_search_website_scrapings (\n",
    "               id INTEGER PRIMARY KEY,\n",
    "               Firm_Name TEXT NOT NULL,\n",
    "               Registered_Address TEXT,\n",
    "               CEO TEXT,\n",
    "               Establishment_Year TEXT,\n",
    "               Number_Of_Employees TEXT,\n",
    "               Revenue_Size TEXT,\n",
    "               Website TEXT,\n",
    "               NAICS_Code TEXT,\n",
    "               SIC_Code TEXT,\n",
    "               Status TEXT,\n",
    "               Dissolvement_Year TEXT,\n",
    "               Company_Type TEXT,\n",
    "               Previous_Names TEXT,\n",
    "               Alternative_Names TEXT,\n",
    "               Key_Executive_Personnel TEXT\n",
    "               )\n",
    "               ''')\n",
    "\n",
    "conn_websites.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Execute Main Loop to construct the database\n",
    "- Skip over existing entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize 5 webscraper class instances for parallel processing\n",
    "scrapers = [WebScraper() for _ in range(5)]\n",
    "site_scraper = scrapers[0]\n",
    "\n",
    "# Get all firms from the web search results database\n",
    "cursor_websearch.execute(''' SELECT id, Firm_Name FROM firms_web_search_results ''')\n",
    "firm_web_search_results = cursor_websearch.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Override with sample firms for demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"firms_sample.csv\")\n",
    "for i,row in df.iterrows():\n",
    "    row = row.to_dict()\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "counter = 0\n",
    "\n",
    "for i,row in df.iterrows():\n",
    "    row = row.to_dict()\n",
    "    firm_id = row['id']\n",
    "    firm_name = row['name']\n",
    "    print(\"----- Debug: Now on firm: \", firm_name, \" -----\", firm_id)\n",
    "\n",
    "    # Check if a row for the firm already exists in the target database, insert row if it doesnt\n",
    "    cursor_websites.execute(\"SELECT id FROM firms_web_search_website_scrapings WHERE id = ? AND Firm_Name = ?\", (firm_id, firm_name,))\n",
    "    firm_row = cursor_websites.fetchone()\n",
    "    if firm_row is None:\n",
    "        print(\"Inserting new firm:, \", firm_name)\n",
    "        cursor_websites.execute(\"INSERT INTO firms_web_search_website_scrapings (id, Firm_Name) VALUES (?,?)\", (firm_id,firm_name,))\n",
    "        firm_id = cursor_websites.lastrowid  \n",
    "    else:\n",
    "        # print(\"Found row for firm, \", firm_name)\n",
    "        firm_id = firm_row[0]\n",
    "\n",
    "    # Now iterate through each field's search results for the given firm\n",
    "    for field in fields:\n",
    "        print(\"Debug field is \", field, \" for firm \", firm_name , \" and id \", firm_id)\n",
    "\n",
    "        # Check if the field value in the target database is NULL to decide if we need to fill it in\n",
    "        cursor_websites.execute(f\"SELECT {field} FROM firms_web_search_website_scrapings WHERE id = ? AND Firm_Name = ? AND {field} IS NOT NULL\", (firm_id,firm_name,))\n",
    "        existing_result = cursor_websites.fetchone()\n",
    "        # If the field already has a value, skip this iteration\n",
    "        if existing_result is not None and existing_result[0] != '{}':\n",
    "            print(f\"Field '{field}' already has data for firm '{firm_name}', skipping.\")\n",
    "            continue\n",
    "      \n",
    "        #get the websearch results\n",
    "        cursor_websearch.execute(f\"SELECT {field} FROM firms_web_search_results WHERE id = ? AND Firm_Name = ?\", (firm_id, firm_name,))\n",
    "        web_search_result = cursor_websearch.fetchone()\n",
    "\n",
    "        # check that bing web search results are actually available\n",
    "        if (web_search_result is not None) and (web_search_result[0] is not None):\n",
    "            web_search_result = json.loads(web_search_result[0])\n",
    "        else:\n",
    "            web_search_result = \"No web search data available\" # skip the iteration, nothing to do\n",
    "            continue\n",
    "\n",
    "        # Get the website URLs from the web search results\n",
    "        try:\n",
    "            # print(\"web_search_result is \", web_search_result)\n",
    "            # skip failed search\n",
    "            if web_search_result == \"Bing Search has failed\" or web_search_result is None:\n",
    "                print(\" Skipping failed search, for field \", field)\n",
    "                continue\n",
    "\n",
    "            # Get the top 5 webpages from the search results\n",
    "            webpages = site_scraper.get_top_webpages(web_search_result)\n",
    "            website_info = {} # initialize dictionary to fill\n",
    "\n",
    "            # Use paralllism to scrape the 5 websites simultaneous with 5 Selenium instances\n",
    "\n",
    "            with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "                # Submit each website scrape task to the executor with a unique SiteScraper instance\n",
    "                future_to_site = {\n",
    "                    executor.submit(scrape_website, scrapers[i % 5], name, url): name\n",
    "                    for i, (name, url) in enumerate(webpages.items())\n",
    "                }\n",
    "\n",
    "                # Collect results as they complete\n",
    "                for future in concurrent.futures.as_completed(future_to_site):\n",
    "                    website_name = future_to_site[future]\n",
    "                    try:\n",
    "                        name, result = future.result()\n",
    "                        website_info[name] = result\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error scraping {website_name}: {e}\")\n",
    "\n",
    "            website_info = json.dumps(website_info)\n",
    "\n",
    "            # Update cell value in database\n",
    "            cursor_websites.execute(f\"\"\"\n",
    "                        UPDATE firms_web_search_website_scrapings\n",
    "                        SET {field} = ?\n",
    "                        WHERE id = ? AND Firm_Name = ?\n",
    "                        \"\"\", (website_info, firm_id, firm_name))\n",
    "            \n",
    "            conn_websites.commit()\n",
    "            counter += 1\n",
    "            print(f\"****Successfully updated {field} for {firm_name}), counter at {counter} ***\")\n",
    "\n",
    "        except SeleniumExtractionError as e:\n",
    "            print(f\"Error extracting data for {firm_name} and {field}: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = df.name.tolist()\n",
    "names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Display some results for Demo purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = cursor_websites.execute(\"SELECT * FROM firms_web_search_website_scrapings WHERE Firm_Name IN ({})\".format(\n",
    "    \",\".join([\"?\"] * len(df))\n",
    "), names).fetchall()\n",
    "data_dict = json.loads(data[0][2]) # registered address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # close all cursosrs and connections\n",
    "cursor_websearch.close()\n",
    "cursor_websites.close()\n",
    "conn_websearch.close()\n",
    "conn_websites.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze lengths of results to determine a suitable token cutoff point (save on costs during trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the results\n",
    "data = []\n",
    "lengths = []\n",
    "\n",
    "# Get all firms from the database\n",
    "cursor_websites.execute('SELECT id, Firm_Name FROM firms_web_search_website_scrapings WHERE id > 2000')\n",
    "firm_records = cursor_websites.fetchall()\n",
    "\n",
    "for firm_record in firm_records:\n",
    "    firm_id = firm_record[0]\n",
    "    firm_name = firm_record[1]\n",
    "    firm_data = {'id': firm_id, 'Firm_Name': firm_name}\n",
    "    \n",
    "    print(f\"Processing firm: {firm_name} (ID: {firm_id})\")\n",
    "    \n",
    "    for field in fields:\n",
    "        # Fetch the content of the field\n",
    "        cursor_websites.execute(f'''\n",
    "            SELECT {field} FROM firms_web_search_website_scrapings\n",
    "            WHERE id = ? AND Firm_Name = ?\n",
    "        ''', (firm_id, firm_name))\n",
    "        \n",
    "        result = cursor_websites.fetchone()\n",
    "        if result:\n",
    "            content = result[0]\n",
    "            if content:\n",
    "                try:\n",
    "                    # Parse the JSON content if necessary\n",
    "                    content_json = json.loads(content)\n",
    "                    # Flatten the JSON to a string\n",
    "                    content_str = json.dumps(content_json)\n",
    "                    content_length = len(content_str)\n",
    "                except json.JSONDecodeError:\n",
    "                    # If content is not JSON, treat it as a string\n",
    "                    content_length = len(content)\n",
    "            else:\n",
    "                content_length = 0\n",
    "        else:\n",
    "            content_length = 0\n",
    "        \n",
    "        # Add the length to the firm_data dictionary\n",
    "        firm_data[field] = content_length\n",
    "        lengths.append(content_length)\n",
    "    \n",
    "    # Append the firm_data to the data list\n",
    "    data.append(firm_data)\n",
    "\n",
    "# Convert the data list to a pandas DataFrame\n",
    "df_lengths = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lengths.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths_series = pd.Series(lengths)\n",
    "\n",
    "percentiles = [50, 75, 90, 91,92,93,94, 95, 96, 97, 98, 99]\n",
    "\n",
    "combined_percentiles = lengths_series.quantile([p / 100 for p in percentiles])\n",
    "\n",
    "print(f\"\\nPercentiles across all fields:\")\n",
    "print(combined_percentiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths_series[lengths_series < 1000000].mean()/ 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = lengths_series[lengths_series < 976996.73 ]\n",
    "int(v.sum() / 4) /1000000 * 1.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor_websearch.close()\n",
    "conn_websearch.close()\n",
    "cursor_websites.close()\n",
    "conn_websites.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "equifax_practicum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
