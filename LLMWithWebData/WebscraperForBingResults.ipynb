{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "load_dotenv()\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.exceptions import Timeout, RequestException\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\"Registered_Address\", \"CEO\", \"Establishment_Year\", \"Number_Of_Employees\", \"Revenue_Size\" ,\n",
    "        \"Website\", \"NAICS_Code\", \"SIC_Code\", \"Status\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class for selenium webscraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeleniumExtractionError(Exception):\n",
    "    \"\"\"Custom exception for Selenium extraction errors.\"\"\"\n",
    "    pass\n",
    "\n",
    "class WebScraper():\n",
    "    def __init__(self) -> None:\n",
    "        # Set up Chrome driver with webdriver manager\n",
    "        self.options = webdriver.ChromeOptions()\n",
    "        self.options.add_argument('--headless')  # Run headless for no browser window\n",
    "        self.options.add_argument('--disable-gpu')  # Disable GPU acceleration\n",
    "        # self.options.add_argument('--no-sandbox')  # Required for some Linux environments\n",
    "        self.options.add_argument('--disable-extensions')\n",
    "        self.options.add_argument('--disable-plugins')\n",
    "        self.options.add_argument('--disable-images')  # Prevent loading images to save bandwidth\n",
    "        self.options.add_argument('--disable-browser-side-navigation')\n",
    "        self.options.add_argument('--mute-audio') \n",
    "        self.options.page_load_strategy = 'eager'  \n",
    "\n",
    "        # Automatically download and use ChromeDriver\n",
    "        self.driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=self.options)\n",
    "        self.driver.set_page_load_timeout(4)\n",
    "        \n",
    "        # Paremeters for requests\n",
    "        self.requests_headers =  {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "        }\n",
    "        self.requests_timeout = 4\n",
    "\n",
    "        self.n_webpages_to_scrape = 5\n",
    "        self.webpages = {}\n",
    "\n",
    "\n",
    "    def get_top_webpages(self,web_search_results: dict) -> list:\n",
    "        self.webpages = {}\n",
    "        print(\"Getting urls of top webpages\")\n",
    "    \n",
    "        # get dict of site names and urls\n",
    "        if 'webPages' not in web_search_results.keys():\n",
    "            print(\"No webpages found\")\n",
    "            return self.webpages\n",
    "        else:\n",
    "            for result in web_search_results['webPages']['value']:\n",
    "                # print(\"Result is \", result)\n",
    "                # print(result[\"siteName\"])\n",
    "                if \"siteName\" in result.keys():\n",
    "                    self.webpages[result[\"siteName\"]] = result[\"url\"]\n",
    "                elif \"name\" in result.keys():\n",
    "                    self.webpages[result[\"name\"]] = result[\"url\"]\n",
    "                else:\n",
    "                    pass\n",
    "                if len(self.webpages) >= self.n_webpages_to_scrape:\n",
    "                    break\n",
    "                \n",
    "            # print(\"debug: length of webpages is \", len(self.webpages))\n",
    "            return self.webpages\n",
    "        \n",
    "    def extract_text_with_selenium(self,url):\n",
    "        try:\n",
    "            # Open the URL in the browser\n",
    "            print(\"Selenium DEBUG: getting url\")\n",
    "            # self.driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=self.options)\n",
    "            # self.driver.set_page_load_timeout(5)\n",
    "            self.driver.get(url)\n",
    "            time.sleep(0.1)\n",
    "\n",
    "            # Wait for the page body to be present (max 5 seconds)\n",
    "            WebDriverWait(self.driver, 5).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
    "            print(\"Selenium DEBUG: found body\")\n",
    "            # Get page source and parse it after it has fully loaded\n",
    "            page_source = self.driver.page_source\n",
    "            print(\"Selenium DEBUG: found page source\")\n",
    "            soup = BeautifulSoup(page_source, 'html.parser')\n",
    "            text = soup.get_text(separator='\\n')\n",
    "\n",
    "            # self.safe_quit_selenium()\n",
    "            return text\n",
    "        \n",
    "        except TimeoutException:\n",
    "            error_message = \"Page load exceeded time limit of 5 seconds\"\n",
    "            print(f\"An error occurred with Selenium:: {error_message}\")\n",
    "            # self.safe_quit_selenium()\n",
    "            return \"Page contents not loaded in time\"\n",
    "\n",
    "        except Exception as e:\n",
    "            error_message = str(e)\n",
    "            print(f\"An error occurred with Selenium: {error_message}\")\n",
    "            self.safe_quit_selenium()\n",
    "            if \"disconnected: not connected to DevTools\" in error_message:\n",
    "                print(f\"Error occurred: {error_message}\")\n",
    "                raise SeleniumExtractionError(f\"DevTools disconnection error for URL: {url}\")\n",
    "            else:\n",
    "                raise SeleniumExtractionError(f\"An unexpected error occurred while extracting: {error_message}\")\n",
    "    \n",
    "    def safe_quit_selenium(self):\n",
    "        try:\n",
    "            self.driver.quit()\n",
    "        except Exception as e:\n",
    "            print(f\"Error during driver quit: {e}\")\n",
    "        finally:\n",
    "            self.kill_chrome_processes()\n",
    "\n",
    "    def kill_chrome_processes(self):\n",
    "        import psutil\n",
    "        PROCNAME = \"chromedriver\" # or \"chrome\" depending on your setup\n",
    "        for proc in psutil.process_iter():\n",
    "            # check whether the process name matches\n",
    "            if proc.name() == PROCNAME:\n",
    "                proc.kill()\n",
    "            \n",
    "        \n",
    "    def extract_text_with_requests(self,url):\n",
    "        try:\n",
    "            # Fetch the content from the URL with headers\n",
    "            print(\"Requests DEBUG: getting url\")\n",
    "            response = requests.get(url, headers=self.requests_headers, timeout=self.requests_timeout)\n",
    "            response.raise_for_status()  # Check if the request was successful\n",
    "            print(\"Requests DEBUG: parsing text\")\n",
    "            # Parse text\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            text = soup.get_text(separator='\\n')  # Using '\\n' to preserve some structure\n",
    "            return text\n",
    "\n",
    "        except Timeout:\n",
    "            print(f\"Request timed out after 5 seconds for URL: {url}\")\n",
    "            return None\n",
    "        except RequestException as e:\n",
    "            print(f\"Error fetching the URL {url}: {e}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error occurred while processing {url}: {e}\")\n",
    "            return None\n",
    "        \n",
    "    def scrape_webpage(self, url: str) -> str:\n",
    "        # use both methods and return the one that works\n",
    "        # print(\"Extracting webpage with selenium\")\n",
    "        text_1 = self.extract_text_with_selenium(url)\n",
    "        # print(\"Extracting webpage with requests\")\n",
    "        text_2 = self.extract_text_with_requests(url)\n",
    "\n",
    "        if text_1 is None and text_2 is None:\n",
    "            return None\n",
    "        elif text_1 is None:\n",
    "            return text_2\n",
    "        elif text_2 is None:\n",
    "            return text_1\n",
    "        elif len(text_1) > len(text_2):\n",
    "            return text_1\n",
    "        else:\n",
    "            return text_2\n",
    "        \n",
    "    def clean_text(text):\n",
    "        if text is None:\n",
    "            return None\n",
    "        # Replace multiple newlines with a single newline\n",
    "        text = re.sub(r'\\n+', '\\n', text)\n",
    "        # Remove leading/trailing whitespace from each line\n",
    "        text = '\\n'.join(line.strip() for line in text.splitlines())\n",
    "        # Remove extra spaces between words\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to sqlite tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_websearch = sqlite3.connect(\"firms_web_search_results.db\")\n",
    "conn_websites = sqlite3.connect(\"firms_web_search_results.db.db\")\n",
    "cursor_websearch = conn_websearch.cursor()\n",
    "cursor_websites = conn_websites.cursor()\n",
    "\n",
    "cursor_websites.execute('''\n",
    "CREATE TABLE IF NOT EXISTS firm_web_search_website_scrapings (\n",
    "               id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "               Firm_Name TEXT NOT NULL,\n",
    "               Registered_Address TEXT,\n",
    "               CEO TEXT,\n",
    "               Establishment_Year TEXT,\n",
    "               Number_Of_Employees TEXT,\n",
    "               Revenue_Size TEXT,\n",
    "               Website TEXT,\n",
    "               NAICS_Code TEXT,\n",
    "               SIC_Code TEXT,\n",
    "               Status TEXT\n",
    "               )\n",
    "               ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop to construct database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_scraper = WebScraper()\n",
    "start_time = time.time()\n",
    "\n",
    "# Get all firms from the web search results database\n",
    "conn_websearch.execute(''' SELECT id, Firm_Name FROM firms_web_search_results ''')\n",
    "firm_web_search_results = conn_websearch.fetchall()\n",
    "\n",
    "for web_search_result in firm_web_search_results:\n",
    "    firm_id = web_search_result[0]\n",
    "    firm_name = web_search_result[1]\n",
    "\n",
    "    # Check if firm already exists in the target database, insert row if it doesnt\n",
    "    cursor_websites.execute(\"SELECT id FROM firm_web_search_website_scrapings WHERE id = ? AND Firm_Name = ?\", (firm_id, firm_name,))\n",
    "    firm_row = cursor_websites.fetchone()\n",
    "    if firm_row is None:\n",
    "        print(\"Inserting new firm:, \", firm_name)\n",
    "        cursor_websites.execute(\"INSERT INTO firm_web_search_website_scrapings (Firm_Name) VALUES (?)\", (firm_name,))\n",
    "        firm_id = cursor_websites.lastrowid  \n",
    "    else:\n",
    "        # Get the existing firm's id\n",
    "        print(\"Found row for firm, \", firm_name)\n",
    "        firm_id = firm_row[0]\n",
    "\n",
    "    # Now iterate through each field's search results for the given firm\n",
    "\n",
    "    for field in fields:\n",
    "\n",
    "        # Check if the field value in the target database is NULL\n",
    "        cursor_websites.execute(f\"SELECT {field} FROM firm_properties WHERE id = ? AND Firm_Name = ? AND {field} IS NOT NULL\", (firm_id,firm_name,))\n",
    "        if cursor_websites.fetchone() is not None:\n",
    "            print(f\"Field '{field}' already has data for firm '{firm_name}', skipping.\")\n",
    "            continue\n",
    "      \n",
    "        # Target database has no value, so fill in\n",
    "        # Get the websearch results\n",
    "        conn_websearch.execute(f\"SELECT {field} FROM firms_web_search_results WHERE id = ? AND Firm_Name = ?\", (firm_id, firm_name,))\n",
    "        web_search_result = conn_websearch.fetchone()\n",
    "        if web_search_result is not None:\n",
    "            web_search_result = json.loads(web_search_result[0]) # TODO check indexing here\n",
    "        else:\n",
    "            web_search_result = \"No web search data available\"\n",
    "            continue\n",
    "\n",
    "        # Get the website URLs from the web search results\n",
    "\n",
    "        try:\n",
    "            webpages = site_scraper.get_top_webpages(web_search_result)\n",
    "            website_info = {}\n",
    "\n",
    "            for website_name, website_url in webpages.items():\n",
    "                print(f\"Getting Contents of the website of {website_name} with url {website_url}, t = {round(time.time() - start_time,2)}\")\n",
    "                result = site_scraper.scrape_webpage(website_url)\n",
    "                website_info[website_name] = result\n",
    "\n",
    "            website_info = json.dumps(website_info)\n",
    "\n",
    "            # Update cell value in database\n",
    "            cursor_websites.execute(f\"\"\"\n",
    "                        UPDATE firms_web_search_results\n",
    "                        SET {field} = ?\n",
    "                        WHERE id = ? AND Firm_Name = ?\n",
    "                        \"\"\", (website_info, firm_id, firm_name))\n",
    "            \n",
    "            conn_websites.commit()\n",
    "\n",
    "        except SeleniumExtractionError as e:\n",
    "            print(f\"Error extracting data for {firm_name} and {field}: {e}\")\n",
    "            continue\n",
    "\n",
    "\n",
    "cursor_websearch.close()\n",
    "conn_websearch.close()\n",
    "cursor_websites.close()\n",
    "conn_websites.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "equifax_practicum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
